{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkers.game import Game\n",
    "import chess\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, buffer_size):\n",
    "\n",
    "        # total size of the replay buffer\n",
    "        self.total_size = buffer_size\n",
    "\n",
    "        # create a list to store the transitions\n",
    "        self._data_buffer = []\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data_buffer)\n",
    "\n",
    "    def add(self, state, reward, policy):\n",
    "        # create a tuple\n",
    "        trans = (state, reward, policy)\n",
    "\n",
    "        # interesting implementation\n",
    "        if self._next_idx >= len(self._data_buffer):\n",
    "            self._data_buffer.append(trans)\n",
    "        else:\n",
    "            self._data_buffer[self._next_idx] = trans\n",
    "\n",
    "        # increase the index\n",
    "        self._next_idx = (self._next_idx + 1) % self.total_size\n",
    "\n",
    "    def _encode_sample(self, indices):\n",
    "        # lists for transitions\n",
    "        state_list, rewards_list, policy_list = [], [], []\n",
    "\n",
    "        # collect the data\n",
    "        for idx in indices:\n",
    "            # get the single transition\n",
    "            data = self._data_buffer[idx]\n",
    "            state, reward, policy = data\n",
    "            # store to the list\n",
    "            state_list.append(np.array(state, copy=False))\n",
    "            rewards_list.append(np.array(reward, copy=False))\n",
    "            policy_list.append(np.array(policy, copy=False))\n",
    "        # return the sampled batch data as numpy arrays\n",
    "        return np.array(state_list), np.array(rewards_list), np.array(policy_list)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        # sample indices with replaced\n",
    "        indices = [np.random.randint(0, len(self._data_buffer)) for _ in range(batch_size)]\n",
    "        return self._encode_sample(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckersEnv(object):\n",
    "    def __init__(self):\n",
    "        self.game = Game()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.game = Game()\n",
    "\n",
    "    def diagonalPosToCord(self, diagonal_pos):\n",
    "        pos_val = diagonal_pos * 2 - 1\n",
    "        row = (pos_val) // 8\n",
    "        col = (pos_val) % 8 - (row) % 2\n",
    "\n",
    "        return row, col\n",
    "\n",
    "    def getAvailableMoves(self, game = None):\n",
    "        chosenGame = self.game\n",
    "        if game:\n",
    "            chosenGame = game\n",
    "\n",
    "        moves = []\n",
    "        for move in chosenGame.get_possible_moves():\n",
    "            moves.append((move[0],move[1]))\n",
    "        return moves\n",
    "    \n",
    "    def swapPlayer(self, player):\n",
    "        if player == 1:\n",
    "            return 2\n",
    "        return 1\n",
    "\n",
    "    #returns Correct, S', R, Done\n",
    "    def step(self, action, player, game = None, tupleify = False):\n",
    "        chosenGame = self.game\n",
    "        if game:\n",
    "            chosenGame = game\n",
    "\n",
    "        if chosenGame.whose_turn() != player:\n",
    "            print(\"NOT MY TURN!\")\n",
    "            return -1,None,None,None\n",
    "        \n",
    "        if action not in self.getAvailableMoves(game):\n",
    "            print(\"NOT POSSIBLE MOVE!\")\n",
    "            return -2,None,None,None\n",
    "        \n",
    "        chosenGame.move([action[0],action[1]])\n",
    "\n",
    "\n",
    "        done = chosenGame.is_over()\n",
    "        reward = 0 #Draws are not punished at the moment\n",
    "        if done:\n",
    "            reward = 1 if chosenGame.get_winner() == player else -1\n",
    "\n",
    "        if tupleify:\n",
    "            s_prime = self.tupleifyBoard(game = chosenGame)\n",
    "        else:\n",
    "            s_prime = self.npifyBoard(game = chosenGame)\n",
    "\n",
    "        return 0, s_prime, reward, done, chosenGame.whose_turn()\n",
    "    \n",
    "    #create_new_board_from_move\n",
    "    def npifyBoard(self, game = None):\n",
    "        chosenGame = self.game\n",
    "        if game:\n",
    "            chosenGame = game\n",
    "        npBoard = np.zeros((8,8))\n",
    "\n",
    "        for piece in chosenGame.board.pieces:\n",
    "            if piece.captured:\n",
    "                continue\n",
    "\n",
    "            row, col = self.diagonalPosToCord(piece.position)\n",
    "\n",
    "            npBoard[row][col] = (1 if piece.player == 1 else -1) * (2 if piece.king else 1)\n",
    "        return npBoard\n",
    "    \n",
    "    def tupleifyBoard(self, game = None):\n",
    "        return tuple(map(tuple, self.npifyBoard(game)))\n",
    "\n",
    "    #there are 32 valid positions: and due to capture chains we can have 31 valid destinations\n",
    "    #most of these comboes of start and end will be impossible, but that can be handled by mcts I suppose\n",
    "    def actionToNNIndex(self, a):\n",
    "        start = a[0]-1\n",
    "        end = a[1]\n",
    "        return start * 32 + end\n",
    "    \n",
    "\n",
    "    def nnifyBoard(self, state, player):\n",
    "        nnifiedBoard = np.zeros((1,4,8,8))\n",
    "        board = state\n",
    "        \n",
    "        if player == 2:\n",
    "            board = np.rot90(board, 2)\n",
    "            board *= -1\n",
    "\n",
    "        for i in range(len(board)):\n",
    "            for j in range(len(board[i])):\n",
    "                if board[i][j] == 1:\n",
    "                    nnifiedBoard[0][0][i][j] = 1\n",
    "                elif board[i][j] == 2:\n",
    "                    nnifiedBoard[0][1][i][j] = 1\n",
    "                elif board[i][j] == -1:\n",
    "                    nnifiedBoard[0][2][i][j] = 1\n",
    "                elif board[i][j] == -2:\n",
    "                    nnifiedBoard[0][3][i][j] = 1\n",
    "\n",
    "        return torch.tensor(nnifiedBoard,dtype=torch.float32)\n",
    "\n",
    "    def nnifyActionProbs(self, actions, probabilities):\n",
    "        probs = np.zeros((32*32,))\n",
    "\n",
    "        for i in range(len(actions)):\n",
    "            probs[self.actionToNNIndex(actions[i])] = probabilities[i]\n",
    "\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSCheckers(object):\n",
    "    def __init__(self, num_searches, env, net, c):\n",
    "        self.env = env\n",
    "        self.game = env.game\n",
    "        self.num_searches = num_searches\n",
    "        self.c = c\n",
    "\n",
    "        self.net = net\n",
    "\n",
    "    def simulate(self, player):\n",
    "        rootS = self.env.tupleifyBoard()\n",
    "        self.Q = {}\n",
    "        self.N = {rootS: 0}\n",
    "        self.Na = {}\n",
    "        self.V = {}\n",
    "        initP, _ = self.net(self.env.nnifyBoard(rootS,player))\n",
    "        self.P = {rootS: initP.detach().numpy()[0]}\n",
    "\n",
    "        for b in range(self.num_searches):\n",
    "            game = deepcopy(self.env.game)\n",
    "            p = player\n",
    "            self.search(rootS, p, game)\n",
    "\n",
    "        valids = self.env.getAvailableMoves()\n",
    "        visits = [self.Na[(rootS, a)] if (rootS, a) in self.Na else 0 for a in valids]\n",
    "        sum_v = float(sum(visits))\n",
    "        return [a / sum_v for a in visits], valids\n",
    "\n",
    "    def search(self, s, player, game, id=0, return_multiplier=-1):\n",
    "        if s not in self.P: #leaf node\n",
    "            p_vals, v = self.net(self.env.nnifyBoard(s, player))\n",
    "            self.P[s] = p_vals.detach().numpy()[0]\n",
    "            self.N[s] = 0\n",
    "\n",
    "            return return_multiplier*v\n",
    "        \n",
    "\n",
    "        best, best_a = None, None\n",
    "        for action in self.env.getAvailableMoves(game):\n",
    "            if (s,action) not in self.Na.keys():\n",
    "                Na = 0\n",
    "            else:\n",
    "                Na = self.Na[(s,action)]\n",
    "\n",
    "            if s not in self.N.keys():\n",
    "                N = 0\n",
    "            else:\n",
    "                N = self.N[s]\n",
    "\n",
    "            if (s,action) in self.Q.keys():\n",
    "                a_to_index = self.env.actionToNNIndex(action)\n",
    "                UCB = self.Q[(s,action)] + self.c * self.P[s][a_to_index]*math.sqrt(N)/(1 + Na)\n",
    "            else:\n",
    "                a_to_index = self.env.actionToNNIndex(action)\n",
    "                UCB = self.c * self.P[s][a_to_index] * math.sqrt(N + .000001)\n",
    "            \n",
    "            if best is None or UCB > best:\n",
    "                best = UCB\n",
    "                best_a = action\n",
    "\n",
    "        beforePlayer = player\n",
    "        err, s_prime, r, done, player = self.env.step(best_a, player, game, tupleify=True)\n",
    "        \n",
    "        if(done):\n",
    "            if (s, best_a) in self.Q.keys():\n",
    "                self.Q[(s,best_a)] = (self.Na[(s,best_a)] * self.Q[(s,best_a)] + r)/(self.Na[(s,best_a)]+1)\n",
    "                self.Na[(s,best_a)] += 1\n",
    "            else:\n",
    "                self.Q[(s,best_a)] = r\n",
    "                self.Na[(s,best_a)] = 1\n",
    "\n",
    "            self.N[s] += 1\n",
    "            return return_multiplier*r\n",
    "        else:\n",
    "            if beforePlayer == player:\n",
    "                v = self.search(s_prime, player, game, id=id+1, return_multiplier=1)\n",
    "            else:\n",
    "                v = self.search(s_prime, player, game, id=id+1)\n",
    "            \n",
    "            if (s, best_a) in self.Q.keys():\n",
    "                self.Q[(s,best_a)] = (self.Na[(s,best_a)] * self.Q[(s,best_a)] + v)/(self.Na[(s,best_a)]+1)\n",
    "                self.Na[(s,best_a)] += 1\n",
    "            else:\n",
    "                self.Q[(s,best_a)] = v\n",
    "                self.Na[(s,best_a)] = 1\n",
    "\n",
    "            self.N[s] += 1\n",
    "            return return_multiplier*v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckersConvNet(nn.Module):\n",
    "    def __init__(self, input_channels, policy_output_dim):\n",
    "        super(CheckersConvNet, self).__init__()\n",
    "\n",
    "        def _conv_layer(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        \n",
    "\n",
    "        self.conv1 = _conv_layer(input_channels, 64)\n",
    "        self.conv2 = _conv_layer(64, 128)\n",
    "        self.conv3 = _conv_layer(128, 256)\n",
    "        \n",
    "        self.policy_head = nn.Linear(256 * 64, policy_output_dim)  # Flattened size after conv layers\n",
    "        \n",
    "        #value head\n",
    "        self.value_flat = nn.Linear(256 * 64, 128)\n",
    "        self.value_head = nn.Linear(128, 1) #only want a single value output\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        p = self.policy_head(x)\n",
    "        p = F.softmax(p, dim=-1)\n",
    "\n",
    "        v = self.value_flat(x)\n",
    "        v = F.relu(v)\n",
    "        v = self.value_head(v)\n",
    "        v = torch.tanh(v)\n",
    "\n",
    "        return p, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSStandard(object):\n",
    "    def __init__(self, num_searches, env, c):\n",
    "        self.env = env\n",
    "        self.game = env.game\n",
    "        self.num_searches = num_searches\n",
    "        self.c = c\n",
    "\n",
    "    def simulate(self, player):\n",
    "        rootS = self.env.tupleifyBoard()\n",
    "        self.T = {}\n",
    "        self.N = {rootS:0}\n",
    "        self.Na = {}\n",
    "\n",
    "        for b in range(self.num_searches):\n",
    "            game = deepcopy(self.env.game)\n",
    "            p = player\n",
    "            self.search(rootS, p, game)\n",
    "\n",
    "        valids = self.env.getAvailableMoves()\n",
    "        visits = [self.Na[(rootS, a)] if (rootS, a) in self.Na else 0 for a in valids]\n",
    "        sum_v = float(sum(visits))\n",
    "        return [a / sum_v for a in visits], valids\n",
    "\n",
    "    def search(self, s, player, game, id=0, return_multiplier=-1):\n",
    "        if s not in self.N: #leaf node=\n",
    "            self.N[s] = 0\n",
    "            return -self.rollout(player, game)\n",
    "\n",
    "        best, best_a = None, None\n",
    "        for action in self.env.getAvailableMoves(game):\n",
    "            if (s,action) not in self.Na.keys():\n",
    "                Na = 0\n",
    "                T_val = 0\n",
    "            else:\n",
    "                Na = self.Na[(s,action)]\n",
    "                T_val = self.T[(s,action)]\n",
    "            \n",
    "            if s not in self.N.keys():\n",
    "                N = 0\n",
    "            else:\n",
    "                N = self.N[s]\n",
    "\n",
    "            if Na == 0:\n",
    "                Na = .0001\n",
    "\n",
    "            UCB = T_val / Na + math.sqrt((2*math.log(N+1))/Na)\n",
    "            \n",
    "            if best is None or UCB > best:\n",
    "                best = UCB\n",
    "                best_a = action\n",
    "\n",
    "        beforePlayer = player\n",
    "        err, s_prime, r, done, player = self.env.step(best_a, player, game, tupleify=True)\n",
    "        \n",
    "        if(done):\n",
    "            if (s, best_a) in self.T.keys():\n",
    "                self.T[(s,best_a)] += r\n",
    "                self.Na[(s,best_a)] += 1\n",
    "            else:\n",
    "                self.T[(s,best_a)] = r\n",
    "                self.Na[(s,best_a)] = 1\n",
    "\n",
    "            self.N[s] += 1\n",
    "            return return_multiplier*r\n",
    "        else:\n",
    "            if beforePlayer == player:\n",
    "                v = self.search(s_prime, player, game, id=id+1, return_multiplier=1)\n",
    "            else:\n",
    "                v = self.search(s_prime, player, game, id=id+1)\n",
    "            \n",
    "            if (s, best_a) in self.T.keys():\n",
    "                self.T[(s,best_a)] += v\n",
    "                self.Na[(s,best_a)] += 1\n",
    "            else:\n",
    "                self.T[(s,best_a)] = v\n",
    "                self.Na[(s,best_a)] = 1\n",
    "\n",
    "            self.N[s] += 1\n",
    "            return return_multiplier*v\n",
    "        \n",
    "    def rollout(self, player, game):\n",
    "        while True:\n",
    "            action = self.env.getAvailableMoves(game)[random.randint(0,len(self.env.getAvailableMoves(game))-1)]\n",
    "            _, _, reward, done, player = self.env.step(action, player, game, tupleify=True)\n",
    "\n",
    "            if done:\n",
    "                reward = reward if player == 1 else -reward\n",
    "                return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_to_tensor(batch_data):\n",
    "    # store the tensor\n",
    "    batch_data_tensor = {'state': [], 'reward': [], 'policy': []}\n",
    "    # get the numpy arrays\n",
    "    state_arr, reward_arr, policy_arr = batch_data\n",
    "    # convert to tensors\n",
    "    batch_data_tensor['state'] = torch.tensor(state_arr, dtype=torch.float32)\n",
    "    batch_data_tensor['reward'] = torch.tensor(reward_arr, dtype=torch.float32)\n",
    "    batch_data_tensor['policy'] = torch.tensor(policy_arr, dtype=torch.float32)\n",
    "\n",
    "    return batch_data_tensor\n",
    "\n",
    "\n",
    "def trainModel(model, v_loss, p_loss, optim, batch):\n",
    "\n",
    "    batch_data_tensor = _batch_to_tensor(batch)\n",
    "\n",
    "    state_tensor = batch_data_tensor['state']\n",
    "    reward_tensor = batch_data_tensor['reward']\n",
    "    policy_tensor = batch_data_tensor['policy']\n",
    "\n",
    "    state_tensor = torch.reshape(state_tensor, (32, 4, 8, 8))\n",
    "\n",
    "    policy_estimates, value_estimates = model(state_tensor)\n",
    "\n",
    "    value_loss = v_loss(value_estimates, reward_tensor)\n",
    "\n",
    "    policy_loss = p_loss(policy_estimates, policy_tensor)\n",
    "\n",
    "    total_loss = value_loss + policy_loss\n",
    "\n",
    "    optim.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    return (policy_loss + value_loss).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCheckers(object):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def simulate(self, player):\n",
    "\n",
    "        valids = self.env.getAvailableMoves()\n",
    "        return [1 / len(valids) for a in valids], valids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModels(baseline, comparison, matchCount, env):\n",
    "    results = []\n",
    "    for _ in range(matchCount):\n",
    "        env.reset()\n",
    "        player = 1\n",
    "        while True:\n",
    "            if player == 1:\n",
    "                probs, actions = baseline.simulate(player)\n",
    "            else:\n",
    "                probs, actions = comparison.simulate(player)\n",
    "            action = random.choices(actions, weights=probs, k=1)[0]\n",
    "            _, _, reward, done, player = env.step(action, player)\n",
    "            if done:\n",
    "                reward = reward if player == 1 else -reward\n",
    "                results.append(reward)\n",
    "                break\n",
    "\n",
    "    return results\n",
    "\n",
    "def trainAndSelfPlay(total_episodes, trainFreq, evalFreq, num_searches, c_constant, lr, weight_decay, batch_size, checkpoint, training_epochs = 5):\n",
    "    env = CheckersEnv()\n",
    "    net = CheckersConvNet(4, 32*32)\n",
    "    oldNet = CheckersConvNet(4, 32*32)\n",
    "    oldNet.load_state_dict(net.state_dict())\n",
    "    mcts = MCTSCheckers(num_searches, env, net, c_constant)\n",
    "    oldMCTS = MCTSCheckers(num_searches, env, oldNet, c_constant)\n",
    "    ranModel = RandomCheckers(env)\n",
    "    standardMonte = MCTSStandard(num_searches, env, c_constant)\n",
    "\n",
    "    p_loss = nn.CrossEntropyLoss()\n",
    "    v_loss = nn.MSELoss()\n",
    "    optim = torch.optim.Adam(lr= lr, weight_decay=weight_decay, params=net.parameters())\n",
    "\n",
    "    replay_buffer = ReplayBuffer(50000) #hard coded buffer size\n",
    "\n",
    "    episode_lens = []\n",
    "    training_loss = []\n",
    "    winrates_past = []\n",
    "    winrates_random = []\n",
    "\n",
    "    for episode_t in range(total_episodes):\n",
    "        env.reset()\n",
    "        player = 1\n",
    "\n",
    "        positions = [env.npifyBoard()]\n",
    "        policies = []\n",
    "        players = []\n",
    "        moves = 0\n",
    "        print(\"STARTED EPISODE:\", episode_t)\n",
    "        while True:\n",
    "            probs, actions = mcts.simulate(player)\n",
    "            \n",
    "            policies.append(env.nnifyActionProbs(actions, probs))\n",
    "            players.append(player)\n",
    "\n",
    "            #sample random move\n",
    "            action = random.choices(actions, weights=probs, k=1)[0]\n",
    "\n",
    "            status, new_state, reward, done, player = env.step(action, player)\n",
    "\n",
    "            if done:\n",
    "                reward = reward if player == 1 else -reward\n",
    "                break\n",
    "\n",
    "            positions.append(new_state)\n",
    "\n",
    "            if status != 0:\n",
    "                print('SOMETHING WENT WRONG WHEN TAKING A MOVE!')\n",
    "                continue\n",
    "\n",
    "            moves += 1\n",
    "\n",
    "        episode_lens.append(moves)\n",
    "\n",
    "        print(\"ENDED EPISODE:\", episode_t, \"WITH\",moves,\"MOVES AND PLAYER 1 RESULT\",reward)\n",
    "\n",
    "        player = 1\n",
    "        for i in range(len(positions)):\n",
    "            replay_buffer.add(env.nnifyBoard(positions[i],players[i]),reward * (-1)**i,policies[i])\n",
    "\n",
    "\n",
    "        if episode_t%trainFreq == 0 and episode_t != 0:\n",
    "            for i in range(training_epochs):\n",
    "                batch = replay_buffer.sample_batch(batch_size)\n",
    "                loss = trainModel(net, v_loss, p_loss, optim, batch)\n",
    "                training_loss.append(loss)\n",
    "\n",
    "\n",
    "        if episode_t%evalFreq == 0 and episode_t != 0:\n",
    "            print(\"EVALUATING OLD AlphaZero, and MCTS\")\n",
    "            alphaResults = evaluateModels(oldMCTS, mcts,11,env)\n",
    "            print(\"ALPHA RESULTS: \",alphaResults)\n",
    "\n",
    "            winrate = (alphaResults.count(0)*.5 + alphaResults.count(-1))/11\n",
    "            winrates_past.append(winrate)\n",
    "\n",
    "            if alphaResults.count(1) < alphaResults.count(-1):\n",
    "                print(\"OLD ALPHAZERO NETWORK IS UPDATED\")\n",
    "                oldNet.load_state_dict(net.state_dict())\n",
    "            ranResults = evaluateModels(standardMonte, mcts, 11, env)\n",
    "            winrate = (ranResults.count(0)*.5 + ranResults.count(-1))/11\n",
    "            print(\"MCTS RESULTS: \",ranResults, winrate)\n",
    "            winrates_random.append(winrate)\n",
    "    \n",
    "    with open('loss.txt', 'w') as file:\n",
    "        for value in training_loss:\n",
    "            file.write(f\"{value}\\n\")\n",
    "\n",
    "    with open('episodelen.txt','w') as file:\n",
    "        for value in episode_lens:\n",
    "            file.write(f\"{value}\\n\")\n",
    "\n",
    "    with open('winrates_past.txt','w') as file:\n",
    "        for value in winrates_past:\n",
    "            file.write(f\"{value}\\n\")\n",
    "\n",
    "    with open('winrates_mcts.txt','w') as file:\n",
    "        for value in winrates_random:\n",
    "            file.write(f\"{value}\\n\")\n",
    "\n",
    "\n",
    "    return training_loss, episode_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 400\n",
    "trainFreq = 3\n",
    "evalFreq = 30\n",
    "num_searches = 60\n",
    "c_constant = 1\n",
    "lr = .001\n",
    "weight_decay = 5e-4\n",
    "batch_size = 32\n",
    "checkpoint = 200\n",
    "train_loss, episode_len = trainAndSelfPlay(total_episodes, trainFreq, evalFreq, num_searches, c_constant, lr, weight_decay, batch_size, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winrates_mcts = []\n",
    "with open('winrates_mcts.txt', 'r') as w:\n",
    "    winrates_mcts = w.read().split('\\n')\n",
    "    winrates_mcts.pop()\n",
    "    print(winrates_mcts)\n",
    "\n",
    "winrates_mcts = [float(x) for x in winrates_mcts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(winrates_mcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winrates_random = []\n",
    "with open('winrates_random.txt', 'r') as w:\n",
    "    winrates_random = w.read().split('\\n')\n",
    "    winrates_random.pop()\n",
    "\n",
    "winrates_random = [float(x) for x in winrates_random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(winrates_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winrates_past = []\n",
    "with open('winrates_past.txt', 'r') as w:\n",
    "    winrates_past = w.read().split('\\n')\n",
    "    winrates_past.pop()\n",
    "\n",
    "winrates_past = [float(x) for x in winrates_past]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(winrates_past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winrates_past2 = []\n",
    "with open('winrates_past2.txt', 'r') as w:\n",
    "    winrates_past2 = w.read().split('\\n')\n",
    "    winrates_past2.pop()\n",
    "winrates_past2 = [float(x) for x in winrates_past2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(winrates_past2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
